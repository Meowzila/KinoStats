{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "165e4191-79f0-46be-a8d2-e63e5da39846",
   "metadata": {},
   "source": [
    "### Imports and Genre List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8410b66-7d6e-4cff-8459-0256ce93aaf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gevent import monkey\n",
    "monkey.patch_socket()\n",
    "import grequests\n",
    "import time\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# List of known genres on Letterboxd.com\n",
    "genre_list = ['action', 'adventure', 'animation', 'comedy', 'crime', 'documentary', 'drama',\n",
    "              'family', 'fantasy', 'history', 'horror', 'music', 'mystery', 'romance',\n",
    "              'science-fiction', 'thriller', 'tv-movie', 'war', 'western']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d46ec02e-f4e0-47b8-b213-59595302fcbd",
   "metadata": {},
   "source": [
    "### Data Scraping Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd3486e1-41ee-4ef0-887e-bfd0a0e2650c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ALL FUNCTIONS NECESSARY FOR SCRAPING PROCESS\n",
    "\n",
    "# Creates the base urls for each genre and to be used for the scraping process\n",
    "def generate_genre_urls():\n",
    "    genre_urls = []\n",
    "    for genre in genre_list:\n",
    "        genre_urls.append(f'https://letterboxd.com/films/ajax/genre/{genre}')\n",
    "    return genre_urls\n",
    "\n",
    "# Grabs total number of films for each genre to calculate how many requests for each genre must be sent out\n",
    "def get_film_num_by_genre():\n",
    "    film_genre_counts = []\n",
    "    urls = generate_genre_urls()\n",
    "    page_reqs = (grequests.get(link) for link in urls)\n",
    "    page_responses = grequests.map(page_reqs)\n",
    "    for r in page_responses:\n",
    "        film_page = BeautifulSoup(r.text, 'lxml')\n",
    "        film_num = re.findall(r'are ([\\d,]+)', str(film_page.find(\n",
    "            'h2', class_='ui-block-heading').text))[0].replace(',', '')\n",
    "        film_genre_counts.append(film_num)\n",
    "    film_num_by_genre = create_genre_dict(film_genre_counts)\n",
    "    return film_num_by_genre\n",
    "\n",
    "# Creates a dictionary that separates the film urls by genre\n",
    "def create_genre_dict(film_num):\n",
    "    film_num_by_genre = {}\n",
    "    for idx in range(len(genre_list)):\n",
    "        film_num_by_genre.update({genre_list[idx]: film_num[idx]})\n",
    "    return film_num_by_genre\n",
    "\n",
    "# Creates list of all pages for each genre (ex. .../action/page/1/ THROUGH .../action/page/235/)\n",
    "def generate_genre_page_urls(film_num_by_genre):\n",
    "    final_genre_dict = {}\n",
    "    for genre in genre_list:\n",
    "        genre_urls = []\n",
    "        total_pages = (int(film_num_by_genre[genre])//72) + 1\n",
    "        url = f'https://letterboxd.com/films/ajax/genre/{genre}/size/small/page/'\n",
    "        for page in range(total_pages):\n",
    "            genre_urls.append(url+str(page))\n",
    "        final_genre_dict.update({genre: genre_urls})\n",
    "    return final_genre_dict\n",
    "\n",
    "# Goes through every page of every film genre (72 films per page) and grabs the url fragment (eg. the-incredible-hulk) for each film\n",
    "def get_film_titles():\n",
    "    film_url_fragments = []\n",
    "    film_num_by_genre = get_film_num_by_genre()\n",
    "    genre_url_dict = generate_genre_page_urls(film_num_by_genre)\n",
    "    for genre in genre_list:\n",
    "        genre_start = time.perf_counter()\n",
    "        print(f'Genre: {genre} ({film_num_by_genre[genre]} films, {(int(film_num_by_genre[genre])//72) + 1} pages)')\n",
    "        page_reqs = (grequests.get(link) for link in genre_url_dict[genre])\n",
    "        page_responses = grequests.map(page_reqs, size=75)\n",
    "        for r in page_responses:\n",
    "            film_page = BeautifulSoup(r.text, 'lxml')\n",
    "            film_url_fragments += re.findall(r'data-film-slug=\"/film/([\\w-]+)/\"', str(film_page.find(\n",
    "                'ul', class_='poster-list -p70 -grid')))\n",
    "        genre_end = (time.perf_counter() - genre_start)\n",
    "        print(f'Len: {len(film_url_fragments)} {genre_end.__round__(2)}s\\n')\n",
    "    return film_url_fragments\n",
    "\n",
    "# Removes duplicate url fragments\n",
    "def remove_duplicates(url_fragments):\n",
    "    return list(dict.fromkeys(url_fragments))\n",
    "\n",
    "# Combines letterboxd url with url fragments to form complete, requestable url\n",
    "def create_full_film_urls(uniq_films):\n",
    "    uniq_urls = []\n",
    "    for url in uniq_films:\n",
    "        uniq_urls.append(\"https://letterboxd.com/film/\" + url)\n",
    "    return uniq_urls\n",
    "\n",
    "# Tries to grab all pieces of information from each url and returns info in a dictionary (for translating data to Pandas Dataframe later)\n",
    "def find_info(url_response):\n",
    "    try:\n",
    "        film_url = re.findall(r'slug=\"/film/([\\w-]+)/\"', str(url_response.find('section', class_='poster-list -p230 -single no-hover el col')))[0]\n",
    "    except Exception:\n",
    "        film_url = None\n",
    "    try:\n",
    "        film_name = url_response.find('h1', class_='headline-1 js-widont prettify').text\n",
    "    except Exception:\n",
    "        film_name = None\n",
    "    try:\n",
    "        film_year = url_response.find('small', class_='number').text\n",
    "    except Exception:\n",
    "        film_year = None\n",
    "    try:\n",
    "        film_rating = re.findall(r'\"ratingValue\":(.+?),', str(url_response.find_all()))[0]\n",
    "    except Exception:\n",
    "        film_rating = None\n",
    "    try:\n",
    "        film_cast = re.findall(r'\"/actor/([\\w-]+)/\"', str(url_response.find('div', class_='cast-list text-sluglist')))\n",
    "    except Exception:\n",
    "        film_cast = None\n",
    "    try:\n",
    "        film_director = url_response.find('span', class_='prettify').text\n",
    "    except Exception:\n",
    "        film_director = None\n",
    "    try:\n",
    "        film_studios = re.findall(r'studio/([\\w-]+)/\"', str(url_response.find('div', id='tab-details')))\n",
    "    except Exception:\n",
    "        film_studios = None\n",
    "    try:\n",
    "        film_country = re.findall(r'country/([\\w-]+)/\"', str(url_response.find('div', id='tab-details')))[0]\n",
    "    except Exception:\n",
    "        film_country = None\n",
    "    try:\n",
    "        film_language = re.findall(r'language/([\\w-]+)/\"', str(url_response.find('div', id='tab-details')))[0]\n",
    "    except Exception:\n",
    "        film_language = None\n",
    "    try:\n",
    "        film_genres = re.findall(r'genre/([\\w-]+)/\"', str(url_response.find('div', class_='text-sluglist capitalize')))\n",
    "    except Exception:\n",
    "        film_genres = None\n",
    "\n",
    "    film_info = {\n",
    "        \"url\": film_url,\n",
    "        \"name\": film_name,\n",
    "        \"year\": film_year,\n",
    "        \"rating\": film_rating,\n",
    "        \"cast\": film_cast,\n",
    "        \"director\": film_director,\n",
    "        \"studios\": film_studios,\n",
    "        \"country\": film_country,\n",
    "        \"language\": film_language,\n",
    "        \"genres\": film_genres\n",
    "    }\n",
    "    return film_info\n",
    "\n",
    "# Requests final list of urls and returns list of dictionaries\n",
    "def send_requests(uniq_urls):\n",
    "    formatted_url_data = []\n",
    "    page_reqs = (grequests.get(link) for link in uniq_urls)\n",
    "    page_responses = grequests.map(page_reqs, size=75)\n",
    "    for r in page_responses:\n",
    "        film_page = BeautifulSoup(r.text, 'lxml')\n",
    "        formatted_url_data.append(find_info(film_page))\n",
    "    return formatted_url_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e28e6cef-84f5-4964-884e-88a68b898d8a",
   "metadata": {},
   "source": [
    "### Preparing Necessary Info for Full Scrape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e223f9-42fa-4341-9aff-31e260ea9f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gets list of film url fragments\n",
    "start = time.perf_counter()\n",
    "\n",
    "film_urls = get_film_titles()\n",
    "\n",
    "fin = (time.perf_counter() - start)\n",
    "print(f'\\nTime to scrape and assemble url list: {fin.__round__(2)}s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c37fdfc9-8913-4fe7-8641-d550f186046a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removes all duplicates urls and creates list of final urls to request\n",
    "start = time.perf_counter()\n",
    "\n",
    "uniq = remove_duplicates(film_urls)\n",
    "uniq_url_list = create_full_film_urls(uniq)\n",
    "\n",
    "fin = (time.perf_counter() - start)\n",
    "print(f'\\nTime to clean list: {fin.__round__(2)}s, New List Length: {len(uniq_url_list)})')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1809fd96-335b-4b27-aaa7-eccd5c8fd89d",
   "metadata": {},
   "source": [
    "### Running the Full Scrape (8-10 Hours)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac0256f9-dd56-4f87-875a-5c3ed9159a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inserts just the first line + header to start the csv file\n",
    "# This isn't actually necessary since headers are defined in the postgresql table before importing anyways\n",
    "header = uniq_url_list[:1]\n",
    "kinodf = pd.DataFrame.from_records(send_requests(header))\n",
    "kinodf.to_csv('films.csv', mode='a', encoding='utf8')\n",
    "\n",
    "\n",
    "# This block stores all of the data in a Pandas Dataframe and writes each block of 1000 entires to csv\n",
    "# In retrospect, moving the data between Dataframe and csv is unnecessary, however, this was initially done to better understand what the scraped data looked like\n",
    "limit = len(uniq_url_list)\n",
    "x, y = 1, 1001\n",
    "for _ in range((limit//1000)+1):\n",
    "    bookmark = y//1000\n",
    "    start = time.perf_counter()\n",
    "    if y > limit:\n",
    "        y = limit\n",
    "    kinodf = pd.DataFrame.from_records(send_requests(uniq_url_list[x:y]))\n",
    "    kinodf.to_csv('films.csv', mode='a', encoding='utf8', header=False)\n",
    "    print(f'Processed Chunk [{x}:{y}]')\n",
    "    if y != limit:\n",
    "        x += 1000\n",
    "        y += 1000\n",
    "    fin = (time.perf_counter() - start)\n",
    "    print(f'Time to process chunk: {fin.__round__(2)}s (Estimated time left: {(((fin*(limit//1000))-(fin*bookmark))/3600).__round__(2)} hours)\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ef6a275-5797-4d6d-9deb-e5e83588cb98",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5276ade9-e1d5-4fbc-8634-b7fc70172b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert all brackets [] to curly brackets {} for SQL array syntax\n",
    "# This conversion is also rather unnecessary as I discovered arrays are definitely not the best way to store data within columns in SQL\n",
    "\n",
    "df = pd.read_csv('clean_films.csv')\n",
    "\n",
    "# Convert [genres] to {genres}\n",
    "lclean = df['genres'].replace(to_replace=r'(\\[)', value='{', regex=True)\n",
    "rclean = lclean.replace(to_replace=r'(])', value='}', regex=True)\n",
    "df['genres'] = rclean\n",
    "\n",
    "# Convert [cast] to {cast}\n",
    "lclean = df['cast'].replace(to_replace=r'(\\[)', value='{', regex=True)\n",
    "rclean = lclean.replace(to_replace=r'(])', value='}', regex=True)\n",
    "df['cast'] = rclean\n",
    "\n",
    "# Convert [studios] to {studios}\n",
    "lclean = df['studios'].replace(to_replace=r'(\\[)', value='{', regex=True)\n",
    "rclean = lclean.replace(to_replace=r'(])', value='}', regex=True)\n",
    "df['studios'] = rclean\n",
    "\n",
    "df.to_csv('cleaner_films.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a4cdc7-ae91-45e9-8d3b-13d28aa151f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "\n",
    "def chainer(s):\n",
    "    return list(chain.from_iterable(s.str.split(',')))\n",
    "\n",
    "\"\"\"\n",
    "This is more like pseudo-normalization because primary keys are not actually used in the final database at the moment\n",
    "The resulting data will look like:\n",
    "\n",
    "GenreID     genre\n",
    "0      'action'\n",
    "0      'adventure'\n",
    "1      'drama'\n",
    "2      'documentary'\n",
    "\n",
    "So that the GenreID matches the main FilmID column in the main table and the tables can be joined in queries\n",
    "\"\"\"\n",
    "\n",
    "# NORMALIZE GENRES\n",
    "df = pd.read_csv('genres.csv')\n",
    "lclean = df['genres'].replace(to_replace=r'(\\{)', value='', regex=True)\n",
    "rclean = lclean.replace(to_replace=r'(})', value='', regex=True)\n",
    "df['genres'] = rclean\n",
    "lens = df['genres'].str.split(',').map(len)\n",
    "genre_df = pd.DataFrame({'ID': np.repeat(df['ID'], lens),\n",
    "                    'genre': chainer(df['genres'])})\n",
    "\n",
    "# NORMALIZE CAST\n",
    "df = pd.read_csv('cast.csv')\n",
    "lclean = df['cast'].replace(to_replace=r'(\\{)', value='', regex=True)\n",
    "rclean = lclean.replace(to_replace=r'(})', value='', regex=True)\n",
    "df['cast'] = rclean\n",
    "lens = df['cast'].str.split(',').map(len)\n",
    "cast_df = pd.DataFrame({'ID': np.repeat(df['ID'], lens),\n",
    "                    'actor': chainer(df['cast'])})\n",
    "\n",
    "# NORMALIZE STUDIOS\n",
    "df = pd.read_csv('studios.csv')\n",
    "lclean = df['studios'].replace(to_replace=r'(\\{)', value='', regex=True)\n",
    "rclean = lclean.replace(to_replace=r'(})', value='', regex=True)\n",
    "df['studios'] = rclean\n",
    "lens = df['studios'].str.split(',').map(len)\n",
    "studios_df = pd.DataFrame({'ID': np.repeat(df['ID'], lens),\n",
    "                    'studio': chainer(df['studios'])})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87080caf-1b89-4d84-ae71-bac8fb13b3ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WRITE NORMALIZED DFs TO CSV FILES\n",
    "genre_df.to_csv('clean_genres.csv')\n",
    "cast_df.to_csv('clean_cast.csv')\n",
    "studios_df.to_csv('clean_studios.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a50280f3-63b0-411a-82ea-6964c8ff7e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The above \"Normalization\" does not remove the single-quotes around each entry and prevent proper querying within the database\n",
    "# Therefore, each value must be further cleaned and re-written into a final csv for importation\n",
    "fix_genres = pd.read_csv('clean_genres.csv')\n",
    "fix_studios = pd.read_csv('clean_studios.csv')\n",
    "fix_cast = pd.read_csv('clean_cast.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cc74377-8b91-4f8a-893d-c36f55713a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This block removes the singles quotes around each csv entry due to data recognition issues in SQL\n",
    "# 'action' -> action\n",
    "\n",
    "test = []\n",
    "ids = []\n",
    "for genre in range(len(fix_genres)):\n",
    "    try:\n",
    "        test.append(fix_genres['genre'][genre].replace('\\'', '').replace(' ', ''))\n",
    "        ids.append(fix_genres['ID'][genre])\n",
    "    except AttributeError:\n",
    "        test.append(fix_genres['genre'][genre])\n",
    "        ids.append(fix_genres['ID'][genre])\n",
    "new_genre_df = pd.DataFrame({'ID': ids, 'genre': test})\n",
    "\n",
    "test = []\n",
    "ids = []\n",
    "for studio in range(len(fix_studios)):\n",
    "    try:\n",
    "        test.append(fix_studios['studio'][studio].replace('\\'', '').replace(' ', ''))\n",
    "        ids.append(fix_studios['ID'][studio])\n",
    "    except AttributeError:\n",
    "        test.append(fix_studios['studio'][studio])\n",
    "        ids.append(fix_studios['ID'][studio])\n",
    "new_studios_df = pd.DataFrame({'ID': ids, 'studio': test})\n",
    "\n",
    "test = []\n",
    "ids = []\n",
    "for actor in range(len(fix_cast)):\n",
    "    try:\n",
    "        test.append(fix_cast['actor'][actor].replace('\\'', '').replace(' ', ''))\n",
    "        ids.append(fix_cast['ID'][actor])\n",
    "    except AttributeError:\n",
    "        test.append(fix_cast['actor'][actor])\n",
    "        ids.append(fix_cast['ID'][actor])\n",
    "new_cast_df = pd.DataFrame({'ID': ids, 'actor': test})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "456d167f-6842-4880-a1f4-aa07f2632475",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the \"fixed\" csv files\n",
    "new_genre_df.to_csv('fixed_genres.csv')\n",
    "new_studios_df.to_csv('fixed_studios.csv')\n",
    "new_cast_df.to_csv('fixed_cast.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
